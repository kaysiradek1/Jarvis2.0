"""
Full Qwen2.5-VL Deployment on Modal
Production-ready vision AI system
"""
import modal

app = modal.App("qwen-vision-ai")

# Build image with Qwen models
qwen_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git", "curl")
    .pip_install(
        "fastapi[standard]",
        "pillow",
        "numpy",
        "torch==2.2.0",
        "torchvision==0.17.0",
        "transformers @ git+https://github.com/huggingface/transformers",
        "qwen-vl-utils==0.0.8",
        "accelerate",
        "einops",
        "sentencepiece"
    )
    .run_commands(
        # Pre-download model components
        "pip install --upgrade transformers",
        "python -c \"from transformers import AutoProcessor; AutoProcessor.from_pretrained('Qwen/Qwen2.5-VL-7B-Instruct'); print('Processor cached')\""
    )
)

@app.function(
    image=qwen_image,
    gpu="a10g",  # A10G GPU (24GB VRAM) for Qwen
    memory=32768,  # 32GB RAM
    timeout=300,
    min_containers=0,  # Scale to zero when not in use
    max_containers=3,
    container_idle_timeout=60  # Keep warm for 1 minute
)
@modal.web_endpoint(method="POST", label="frame")
def process_frame(request: dict):
    """Process iPhone screen with Qwen2.5-VL"""
    import base64
    import io
    import torch
    from PIL import Image
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    from qwen_vl_utils import process_vision_info
    
    # Initialize model (cached after first load)
    if not hasattr(process_frame, "model"):
        print("Loading Qwen2.5-VL-7B...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        process_frame.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            trust_remote_code=True
        )
        
        process_frame.model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-7B-Instruct",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto",
            trust_remote_code=True
        )
        
        process_frame.device = device
        print(f"‚úÖ Model loaded on {device}")
    
    try:
        # Decode image
        image_data = base64.b64decode(request.get("image", ""))
        image = Image.open(io.BytesIO(image_data))
        
        # Prepare vision prompt
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": """Analyze this iPhone/Android screen:
1. What app or screen is this?
2. List ALL UI elements you see (buttons, text, icons)
3. What actions can be taken?
4. Generate automation commands.

Respond with specific tap/swipe commands with coordinates."""}
            ]
        }]
        
        # Process with Qwen
        text = process_frame.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = process_frame.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(process_frame.device)
        
        # Generate response
        with torch.no_grad():
            generated_ids = process_frame.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.3,
                do_sample=True
            )
        
        # Decode response
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in 
            zip(inputs.input_ids, generated_ids)
        ]
        
        response = process_frame.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # Parse response and generate commands
        commands = []
        width, height = image.size
        
        # Simple parsing - enhance based on actual Qwen output
        response_lower = response.lower()
        
        if "button" in response_lower or "tap" in response_lower:
            commands.append({
                "action": "tap",
                "x": width // 2,
                "y": height // 2,
                "reason": "UI element detected by Qwen"
            })
        
        if "scroll" in response_lower or "list" in response_lower:
            commands.append({
                "action": "swipe",
                "start_x": width // 2,
                "start_y": height * 0.7,
                "end_x": width // 2,
                "end_y": height * 0.3,
                "duration": 0.5,
                "reason": "Scrollable content detected"
            })
        
        if "text" in response_lower or "input" in response_lower:
            commands.append({
                "action": "type",
                "text": "AI input",
                "reason": "Text field detected"
            })
        
        return {
            "success": True,
            "model": "Qwen2.5-VL-7B",
            "understanding": response[:500],  # First 500 chars
            "commands": commands,
            "device": process_frame.device,
            "frame_size": [width, height]
        }
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            "success": False,
            "error": str(e),
            "model": "Qwen2.5-VL-7B"
        }

@app.function(
    image=qwen_image,
    min_containers=1  # Keep health check warm
)
@modal.web_endpoint(method="GET", label="health")  
def health_check():
    """Health check endpoint"""
    import torch
    
    return {
        "status": "healthy",
        "service": "Qwen2.5-VL Vision AI",
        "model": "Qwen/Qwen2.5-VL-7B-Instruct",
        "gpu_available": torch.cuda.is_available(),
        "cuda_device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }

@app.local_entrypoint()
def main():
    print("üöÄ Deploying Qwen2.5-VL Vision AI to Modal")
    print("=" * 60)
    print("üì± Model: Qwen2.5-VL-7B-Instruct")
    print("üñ•Ô∏è  GPU: A10G (24GB VRAM)")
    print("üí∞ Cost: ~$0.000067/second when running")
    print("=" * 60)
    print("\nYour endpoints will be:")
    print("  üîó https://kaysiradek--qwen-vision-ai-health.modal.run")
    print("  üîó https://kaysiradek--qwen-vision-ai-frame.modal.run")